% !TEX root = ../main.tex

\chapter{预计困难及解决方案说明}

% ========== 6.1 技术难点与预计困难 ==========
\section{技术难点与预计困难}

1. 数据的异构性与复杂性：
本研究所采用的数据集包含由不同采样参数导致的异构尺寸时频谱图，如何设计高效的数据加载与批处理机制以适应这种内在的异构性，是一个关键的工程挑战。
此外，数据集中信号类型多样、时频密集且存在相互重叠，这对模型的特征表征与精细分辨能力提出了极高的要求。

2. 模型的高复杂性与超参数调优：
本研究中超参数调优的复杂度较高，涉及模型结构参数（如模块层数）、优化器与学习率调度器参数（如学习率、Warmup轮数）、以及损失函数各部分权重系数等众多变量。
不恰当的超参数配置极易导致模型不收敛或陷入局部最优，因此，高效的超参数寻优策略至关重要。

3. 微小信号检测的瓶颈：
在时频谱图中，大量关键信号（如猝发信号）呈现为时宽与带宽极窄的“小目标”。
此类目标在经过骨干网络的多层下采样后，其特征信息极易被稀释或丢失，是目标检测领域的公认技术瓶颈。
如何有效克服这一瓶颈，确保模型对微小信号的检出率与定位精度，是衡量本研究算法先进性的核心指标。

4. 训练开销与显存瓶颈：
引入Transformer架构将不可避免地带来计算量与显存占用的显著增长。
在有限的硬件资源下，为防止显存溢出而过度减小批处理大小（Batch Size），不仅会严重降低训练效率，还可能损害模型的收敛性能与最终精度。

% ========== 6.2 解决方案 ==========
\section{解决方案}

针对上述可能遇到的困难，本课题预先制定了如下应对策略和解决方案：

1. 将设计并实现一个定制化的数据整理函数（Collate Function）。
该函数能够在构建每个训练批次时，动态地将尺寸相同或相近的样本聚合，从而在不破坏原始数据结构的前提下，实现对异构数据的兼容与高效处理。

2. 将采用迁移学习范式，加载在大型公开数据集上预训练的模型权重作为优化起点，以加速收敛并提升性能。
同时，将借鉴相关领域顶级会议论文中的成熟训练策略与超参数配置，构建一个可靠的初始设定。
采用控制变量法进行系统性的超参数调优，并密切监控训练过程中的损失曲线与验证集性能。
辅以早停（Early Stopping）策略避免不必要的计算开销，并通过对模型输出结果的可视化诊断，为针对性的结构与损失函数调整提供直观依据。

3. 充分利用Deformable DETR的多尺度特征融合能力，确保来自底层网络的高分辨率特征被有效传递至检测头。
发挥可变形注意力机制的优势，引导模型将计算资源精准聚焦于微小信号所在的关键区域。

4. 将优先采用混合精度训练（Automatic Mixed Precision, AMP），在几乎不损失模型精度的前提下，大幅降低显存占用并提升训练速度。
若批大小依然受限，将采用梯度累积技术，以时间换空间，在不增加显存消耗的情况下实现等效的大批次训练效果。
在必要时，可利用实验室的多GPU资源，通过数据并行（Data Parallelism）等分布式训练框架，从根本上扩展可用计算与显存资源，突破单卡硬件瓶颈。